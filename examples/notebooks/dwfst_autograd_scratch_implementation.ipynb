{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d2569af",
      "metadata": {},
      "source": [
        "# PyTorch GTN-Compatible Scratch Implementation\n",
        "\n",
        "This notebook rewrites the original prototype into a cleaner, documented implementation that is **API-oriented toward GTN**.\n",
        "\n",
        "## Goals\n",
        "- Keep a GTN-like API (`Graph`, `linear_graph`, `compose`, `forward_score`, `backward`, ...).\n",
        "- Preserve PyTorch autograd end-to-end.\n",
        "- Keep code readable and easy to extend.\n",
        "\n",
        "## Scope and caveats\n",
        "- This is a pure-Python educational implementation, not a C++/CUDA replacement for speed.\n",
        "- It implements a substantial GTN-compatible subset used by the examples in this repo.\n",
        "- `compose` supports common epsilon behavior, but full production-grade epsilon filtering is more involved.\n",
        "- `forward_score` / `viterbi_score` require acyclic graphs (matching common GTN constraints for these ops).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c406e79",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict, deque\n",
        "from typing import Dict, Iterable, List, Optional, Sequence, Set, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import graphviz\n",
        "\n",
        "# Match GTN epsilon semantics\n",
        "epsilon = -1\n",
        "EPSILON = epsilon\n",
        "\n",
        "TensorLike = Union[float, int, torch.Tensor]\n",
        "\n",
        "\n",
        "def _to_weight_tensor(value: TensorLike, calc_grad: bool) -> torch.Tensor:\n",
        "    \"\"\"Convert value to a scalar torch tensor while preserving autograd links when possible.\"\"\"\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        t = value.reshape(())\n",
        "        if not calc_grad and t.requires_grad:\n",
        "            t = t.detach()\n",
        "        return t\n",
        "    return torch.tensor(float(value), dtype=torch.float32, requires_grad=calc_grad)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Arc:\n",
        "    src: int\n",
        "    dst: int\n",
        "    ilabel: int\n",
        "    olabel: int\n",
        "    weight: torch.Tensor\n",
        "\n",
        "\n",
        "class Graph:\n",
        "    \"\"\"\n",
        "    GTN-like weighted graph/transducer.\n",
        "\n",
        "    Key compatibility points:\n",
        "    - Graph(calc_grad=True)\n",
        "    - add_node(start=False, accept=False)\n",
        "    - add_arc(src, dst, ilabel, olabel=ilabel, weight=0.0)\n",
        "    - num_nodes(), num_arcs(), num_start(), num_accept()\n",
        "    - labels_to_list(input=True), weights_to_list(), set_weights(...)\n",
        "    - item() for scalar graphs\n",
        "    - grad() returns a graph with same topology and gradient weights\n",
        "    - zero_grad()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, calc_grad: bool = True):\n",
        "        self.calc_grad = bool(calc_grad)\n",
        "        self._num_nodes = 0\n",
        "        self._start_nodes: Set[int] = set()\n",
        "        self._accept_nodes: Set[int] = set()\n",
        "        self._arcs: List[Arc] = []\n",
        "\n",
        "    # -------- Node/arc construction --------\n",
        "    def add_node(self, start: bool = False, accept: bool = False) -> int:\n",
        "        idx = self._num_nodes\n",
        "        self._num_nodes += 1\n",
        "        if start:\n",
        "            self._start_nodes.add(idx)\n",
        "        if accept:\n",
        "            self._accept_nodes.add(idx)\n",
        "        return idx\n",
        "\n",
        "    def add_arc(\n",
        "        self,\n",
        "        src: int,\n",
        "        dst: int,\n",
        "        ilabel: int,\n",
        "        olabel: Optional[int] = None,\n",
        "        weight: TensorLike = 0.0,\n",
        "    ) -> None:\n",
        "        if olabel is None:\n",
        "            olabel = ilabel\n",
        "        w = _to_weight_tensor(weight, self.calc_grad)\n",
        "        self._arcs.append(Arc(src, dst, int(ilabel), int(olabel), w))\n",
        "        self._num_nodes = max(self._num_nodes, src + 1, dst + 1)\n",
        "\n",
        "    # -------- Introspection --------\n",
        "    def num_nodes(self) -> int:\n",
        "        return self._num_nodes\n",
        "\n",
        "    def num_arcs(self) -> int:\n",
        "        return len(self._arcs)\n",
        "\n",
        "    def num_start(self) -> int:\n",
        "        return len(self._start_nodes)\n",
        "\n",
        "    def num_accept(self) -> int:\n",
        "        return len(self._accept_nodes)\n",
        "\n",
        "    @property\n",
        "    def start_nodes(self) -> Set[int]:\n",
        "        return set(self._start_nodes)\n",
        "\n",
        "    @property\n",
        "    def accept_nodes(self) -> Set[int]:\n",
        "        return set(self._accept_nodes)\n",
        "\n",
        "    def labels_to_list(self, input: bool = True) -> List[int]:\n",
        "        return [a.ilabel if input else a.olabel for a in self._arcs]\n",
        "\n",
        "    def weights_to_list(self) -> List[float]:\n",
        "        return [float(a.weight.detach().cpu().item()) for a in self._arcs]\n",
        "\n",
        "    def weights_to_numpy(self):\n",
        "        import numpy as np\n",
        "        return np.array(self.weights_to_list(), dtype=np.float32)\n",
        "\n",
        "    def set_weights(self, values: Union[Sequence[float], torch.Tensor]) -> None:\n",
        "        if isinstance(values, torch.Tensor):\n",
        "            vals = values.reshape(-1)\n",
        "        else:\n",
        "            vals = torch.tensor(list(values), dtype=torch.float32)\n",
        "        if vals.numel() != len(self._arcs):\n",
        "            raise ValueError(f\"Expected {len(self._arcs)} weights, got {vals.numel()}\")\n",
        "        for i, arc in enumerate(self._arcs):\n",
        "            arc.weight = _to_weight_tensor(vals[i], self.calc_grad)\n",
        "\n",
        "    def weight(self, i: int) -> float:\n",
        "        return float(self._arcs[i].weight.detach().cpu().item())\n",
        "\n",
        "    # -------- Autograd --------\n",
        "    def _weight_tensors(self) -> List[torch.Tensor]:\n",
        "        return [a.weight for a in self._arcs]\n",
        "\n",
        "    def grad(self) -> \"Graph\":\n",
        "        g = Graph(calc_grad=False)\n",
        "        for i in range(self._num_nodes):\n",
        "            g.add_node(start=(i in self._start_nodes), accept=(i in self._accept_nodes))\n",
        "        for arc in self._arcs:\n",
        "            gw = 0.0 if arc.weight.grad is None else float(arc.weight.grad.detach().cpu().item())\n",
        "            g.add_arc(arc.src, arc.dst, arc.ilabel, arc.olabel, gw)\n",
        "        return g\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for w in self._weight_tensors():\n",
        "            if w.grad is not None:\n",
        "                w.grad.zero_()\n",
        "\n",
        "    # -------- Scalar utility --------\n",
        "    def is_scalar_graph(self) -> bool:\n",
        "        if self._num_nodes != 2 or len(self._arcs) != 1:\n",
        "            return False\n",
        "        a = self._arcs[0]\n",
        "        return (0 in self._start_nodes) and (1 in self._accept_nodes) and a.src == 0 and a.dst == 1 and a.ilabel == epsilon and a.olabel == epsilon\n",
        "\n",
        "    def item(self) -> float:\n",
        "        if not self.is_scalar_graph():\n",
        "            raise ValueError(\"item() is only valid for scalar graphs\")\n",
        "        return float(self._arcs[0].weight.detach().cpu().item())\n",
        "\n",
        "    # -------- Debug/visualization --------\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Graph(nodes={self._num_nodes}, arcs={len(self._arcs)}, start={sorted(self._start_nodes)}, accept={sorted(self._accept_nodes)})\"\n",
        "\n",
        "    def draw(self, filename: str, format: str = \"svg\", view: bool = False, label_map: Optional[Dict[str, int]] = None):\n",
        "        inv = None\n",
        "        if label_map is not None:\n",
        "            inv = {v: k for k, v in label_map.items()}\n",
        "\n",
        "        def fmt_label(x: int) -> str:\n",
        "            if x == epsilon:\n",
        "                return \"ε\"\n",
        "            if inv is not None and x in inv:\n",
        "                return str(inv[x])\n",
        "            return str(x)\n",
        "\n",
        "        dot = graphviz.Digraph(filename)\n",
        "        dot.attr(rankdir=\"LR\")\n",
        "        for i in range(self._num_nodes):\n",
        "            shape = \"doublecircle\" if i in self._accept_nodes else \"circle\"\n",
        "            color = \"blue\" if i in self._start_nodes else \"black\"\n",
        "            dot.node(str(i), str(i), shape=shape, color=color)\n",
        "\n",
        "        for a in self._arcs:\n",
        "            lbl = f\"{fmt_label(a.ilabel)}:{fmt_label(a.olabel)} / {float(a.weight.detach().cpu().item()):.3f}\"\n",
        "            dot.edge(str(a.src), str(a.dst), label=lbl)\n",
        "\n",
        "        dot.render(filename, format=format, view=view, cleanup=True)\n",
        "        return dot\n",
        "\n",
        "\n",
        "# ---------------- Constructors ----------------\n",
        "def scalar_graph(weight: TensorLike, calc_grad: bool = True) -> Graph:\n",
        "    g = Graph(calc_grad=calc_grad)\n",
        "    s = g.add_node(start=True)\n",
        "    t = g.add_node(accept=True)\n",
        "    g.add_arc(s, t, epsilon, epsilon, weight)\n",
        "    return g\n",
        "\n",
        "\n",
        "def linear_graph(m: int, n: int, calc_grad: bool = True) -> Graph:\n",
        "    \"\"\"Create a length-m linear acceptor with n labels per timestep.\"\"\"\n",
        "    g = Graph(calc_grad=calc_grad)\n",
        "    for i in range(m + 1):\n",
        "        g.add_node(start=(i == 0), accept=(i == m))\n",
        "    for t in range(m):\n",
        "        for lab in range(n):\n",
        "            g.add_arc(t, t + 1, lab, lab, 0.0)\n",
        "    return g\n",
        "\n",
        "\n",
        "# ---------------- Graph transforms ----------------\n",
        "def clone(g: Graph) -> Graph:\n",
        "    out = Graph(calc_grad=g.calc_grad)\n",
        "    for i in range(g.num_nodes()):\n",
        "        out.add_node(start=(i in g.start_nodes), accept=(i in g.accept_nodes))\n",
        "    for a in g._arcs:\n",
        "        out.add_arc(a.src, a.dst, a.ilabel, a.olabel, a.weight.clone())\n",
        "    return out\n",
        "\n",
        "\n",
        "def project_input(g: Graph) -> Graph:\n",
        "    out = Graph(calc_grad=g.calc_grad)\n",
        "    for i in range(g.num_nodes()):\n",
        "        out.add_node(start=(i in g.start_nodes), accept=(i in g.accept_nodes))\n",
        "    for a in g._arcs:\n",
        "        out.add_arc(a.src, a.dst, a.ilabel, a.ilabel, a.weight)\n",
        "    return out\n",
        "\n",
        "\n",
        "def project_output(g: Graph) -> Graph:\n",
        "    out = Graph(calc_grad=g.calc_grad)\n",
        "    for i in range(g.num_nodes()):\n",
        "        out.add_node(start=(i in g.start_nodes), accept=(i in g.accept_nodes))\n",
        "    for a in g._arcs:\n",
        "        out.add_arc(a.src, a.dst, a.olabel, a.olabel, a.weight)\n",
        "    return out\n",
        "\n",
        "\n",
        "def remove(g: Graph, ilabel: int = epsilon, olabel: Optional[int] = None) -> Graph:\n",
        "    if olabel is None:\n",
        "        olabel = ilabel\n",
        "    out = Graph(calc_grad=g.calc_grad)\n",
        "    for i in range(g.num_nodes()):\n",
        "        out.add_node(start=(i in g.start_nodes), accept=(i in g.accept_nodes))\n",
        "    for a in g._arcs:\n",
        "        if not (a.ilabel == ilabel and a.olabel == olabel):\n",
        "            out.add_arc(a.src, a.dst, a.ilabel, a.olabel, a.weight)\n",
        "    return out\n",
        "\n",
        "\n",
        "def union(graphs: Sequence[Graph]) -> Graph:\n",
        "    out = Graph(calc_grad=any(g.calc_grad for g in graphs))\n",
        "    for g in graphs:\n",
        "        node_map = {}\n",
        "        for i in range(g.num_nodes()):\n",
        "            node_map[i] = out.add_node(start=(i in g.start_nodes), accept=(i in g.accept_nodes))\n",
        "        for a in g._arcs:\n",
        "            out.add_arc(node_map[a.src], node_map[a.dst], a.ilabel, a.olabel, a.weight)\n",
        "    return out\n",
        "\n",
        "\n",
        "def concat(g1: Graph, g2: Graph) -> Graph:\n",
        "    out = Graph(calc_grad=(g1.calc_grad or g2.calc_grad))\n",
        "\n",
        "    m1 = {}\n",
        "    for i in range(g1.num_nodes()):\n",
        "        m1[i] = out.add_node(start=(i in g1.start_nodes), accept=False)\n",
        "    m2 = {}\n",
        "    for i in range(g2.num_nodes()):\n",
        "        m2[i] = out.add_node(start=False, accept=(i in g2.accept_nodes))\n",
        "\n",
        "    for a in g1._arcs:\n",
        "        out.add_arc(m1[a.src], m1[a.dst], a.ilabel, a.olabel, a.weight)\n",
        "    for a in g2._arcs:\n",
        "        out.add_arc(m2[a.src], m2[a.dst], a.ilabel, a.olabel, a.weight)\n",
        "\n",
        "    for a1 in g1.accept_nodes:\n",
        "        for s2 in g2.start_nodes:\n",
        "            out.add_arc(m1[a1], m2[s2], epsilon, epsilon, 0.0)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def closure(g: Graph) -> Graph:\n",
        "    out = Graph(calc_grad=g.calc_grad)\n",
        "    m = {}\n",
        "    for i in range(g.num_nodes()):\n",
        "        m[i] = out.add_node(start=False, accept=False)\n",
        "    for a in g._arcs:\n",
        "        out.add_arc(m[a.src], m[a.dst], a.ilabel, a.olabel, a.weight)\n",
        "\n",
        "    c = out.add_node(start=True, accept=True)\n",
        "    for s in g.start_nodes:\n",
        "        out.add_arc(c, m[s], epsilon, epsilon, 0.0)\n",
        "    for a in g.accept_nodes:\n",
        "        out.add_arc(m[a], c, epsilon, epsilon, 0.0)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _out_arcs_by_src(g: Graph) -> Dict[int, List[Arc]]:\n",
        "    d = defaultdict(list)\n",
        "    for a in g._arcs:\n",
        "        d[a.src].append(a)\n",
        "    return d\n",
        "\n",
        "\n",
        "def compose(g1: Graph, g2: Graph) -> Graph:\n",
        "    \"\"\"\n",
        "    Simplified epsilon-aware composition suitable for many GTN notebook examples.\n",
        "    \"\"\"\n",
        "    out = Graph(calc_grad=(g1.calc_grad or g2.calc_grad))\n",
        "    o1 = _out_arcs_by_src(g1)\n",
        "    o2 = _out_arcs_by_src(g2)\n",
        "\n",
        "    state_map: Dict[Tuple[int, int], int] = {}\n",
        "    q = deque()\n",
        "\n",
        "    for s1 in g1.start_nodes:\n",
        "        for s2 in g2.start_nodes:\n",
        "            p = (s1, s2)\n",
        "            idx = out.add_node(start=True, accept=(s1 in g1.accept_nodes and s2 in g2.accept_nodes))\n",
        "            state_map[p] = idx\n",
        "            q.append(p)\n",
        "\n",
        "    while q:\n",
        "        u1, u2 = q.popleft()\n",
        "        src_idx = state_map[(u1, u2)]\n",
        "\n",
        "        # 1) consume epsilon-output arcs in g1\n",
        "        for a1 in o1.get(u1, []):\n",
        "            if a1.olabel == epsilon:\n",
        "                v = (a1.dst, u2)\n",
        "                if v not in state_map:\n",
        "                    state_map[v] = out.add_node(start=False, accept=(v[0] in g1.accept_nodes and v[1] in g2.accept_nodes))\n",
        "                    q.append(v)\n",
        "                out.add_arc(src_idx, state_map[v], a1.ilabel, epsilon, a1.weight)\n",
        "\n",
        "        # 2) consume epsilon-input arcs in g2\n",
        "        for a2 in o2.get(u2, []):\n",
        "            if a2.ilabel == epsilon:\n",
        "                v = (u1, a2.dst)\n",
        "                if v not in state_map:\n",
        "                    state_map[v] = out.add_node(start=False, accept=(v[0] in g1.accept_nodes and v[1] in g2.accept_nodes))\n",
        "                    q.append(v)\n",
        "                out.add_arc(src_idx, state_map[v], epsilon, a2.olabel, a2.weight)\n",
        "\n",
        "        # 3) regular label matches\n",
        "        for a1 in o1.get(u1, []):\n",
        "            if a1.olabel == epsilon:\n",
        "                continue\n",
        "            for a2 in o2.get(u2, []):\n",
        "                if a2.ilabel == epsilon:\n",
        "                    continue\n",
        "                if a1.olabel == a2.ilabel:\n",
        "                    v = (a1.dst, a2.dst)\n",
        "                    if v not in state_map:\n",
        "                        state_map[v] = out.add_node(start=False, accept=(v[0] in g1.accept_nodes and v[1] in g2.accept_nodes))\n",
        "                        q.append(v)\n",
        "                    out.add_arc(src_idx, state_map[v], a1.ilabel, a2.olabel, a1.weight + a2.weight)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def intersect(g1: Graph, g2: Graph) -> Graph:\n",
        "    \"\"\"Intersection for acceptors (matches input labels).\"\"\"\n",
        "    p = compose(project_input(g1), project_input(g2))\n",
        "    return project_input(p)\n",
        "\n",
        "\n",
        "# ---------------- Scoring ----------------\n",
        "def _topological_order_or_raise(g: Graph) -> List[int]:\n",
        "    indeg = [0] * g.num_nodes()\n",
        "    out_adj = defaultdict(list)\n",
        "    for a in g._arcs:\n",
        "        out_adj[a.src].append(a.dst)\n",
        "        indeg[a.dst] += 1\n",
        "\n",
        "    q = deque([i for i in range(g.num_nodes()) if indeg[i] == 0])\n",
        "    order = []\n",
        "    while q:\n",
        "        u = q.popleft()\n",
        "        order.append(u)\n",
        "        for v in out_adj[u]:\n",
        "            indeg[v] -= 1\n",
        "            if indeg[v] == 0:\n",
        "                q.append(v)\n",
        "\n",
        "    if len(order) != g.num_nodes():\n",
        "        raise ValueError(\"forward/viterbi currently require acyclic graphs in this implementation\")\n",
        "    return order\n",
        "\n",
        "\n",
        "def _logaddexp(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.logaddexp(a, b)\n",
        "\n",
        "\n",
        "def forward_score(g: Graph) -> Graph:\n",
        "    if g.num_nodes() == 0:\n",
        "        return scalar_graph(torch.tensor(float(\"-inf\"), dtype=torch.float32), calc_grad=g.calc_grad)\n",
        "\n",
        "    order = _topological_order_or_raise(g)\n",
        "    out_arcs = _out_arcs_by_src(g)\n",
        "\n",
        "    neg_inf = torch.tensor(float(\"-inf\"), dtype=torch.float32)\n",
        "    alpha = [neg_inf for _ in range(g.num_nodes())]\n",
        "    for s in g.start_nodes:\n",
        "        alpha[s] = torch.tensor(0.0, dtype=torch.float32)\n",
        "\n",
        "    for u in order:\n",
        "        if torch.isneginf(alpha[u]):\n",
        "            continue\n",
        "        for a in out_arcs.get(u, []):\n",
        "            cand = alpha[u] + a.weight\n",
        "            alpha[a.dst] = cand if torch.isneginf(alpha[a.dst]) else _logaddexp(alpha[a.dst], cand)\n",
        "\n",
        "    acc = [alpha[t] for t in g.accept_nodes if not torch.isneginf(alpha[t])]\n",
        "    if not acc:\n",
        "        out = neg_inf\n",
        "    else:\n",
        "        out = acc[0]\n",
        "        for x in acc[1:]:\n",
        "            out = _logaddexp(out, x)\n",
        "\n",
        "    return scalar_graph(out, calc_grad=g.calc_grad)\n",
        "\n",
        "\n",
        "def viterbi_score(g: Graph) -> Graph:\n",
        "    if g.num_nodes() == 0:\n",
        "        return scalar_graph(torch.tensor(float(\"-inf\"), dtype=torch.float32), calc_grad=g.calc_grad)\n",
        "\n",
        "    order = _topological_order_or_raise(g)\n",
        "    out_arcs = _out_arcs_by_src(g)\n",
        "\n",
        "    neg_inf = torch.tensor(float(\"-inf\"), dtype=torch.float32)\n",
        "    alpha = [neg_inf for _ in range(g.num_nodes())]\n",
        "    for s in g.start_nodes:\n",
        "        alpha[s] = torch.tensor(0.0, dtype=torch.float32)\n",
        "\n",
        "    for u in order:\n",
        "        if torch.isneginf(alpha[u]):\n",
        "            continue\n",
        "        for a in out_arcs.get(u, []):\n",
        "            cand = alpha[u] + a.weight\n",
        "            alpha[a.dst] = cand if torch.isneginf(alpha[a.dst]) else torch.maximum(alpha[a.dst], cand)\n",
        "\n",
        "    acc = [alpha[t] for t in g.accept_nodes if not torch.isneginf(alpha[t])]\n",
        "    out = neg_inf if not acc else torch.stack(acc).max()\n",
        "    return scalar_graph(out, calc_grad=g.calc_grad)\n",
        "\n",
        "\n",
        "def viterbi_path(g: Graph) -> Graph:\n",
        "    \"\"\"Returns one best path as a new graph.\"\"\"\n",
        "    order = _topological_order_or_raise(g)\n",
        "    out_arcs = _out_arcs_by_src(g)\n",
        "\n",
        "    neg_inf = torch.tensor(float(\"-inf\"), dtype=torch.float32)\n",
        "    score = [neg_inf for _ in range(g.num_nodes())]\n",
        "    bp: List[Optional[Tuple[int, Arc]]] = [None for _ in range(g.num_nodes())]\n",
        "\n",
        "    for s in g.start_nodes:\n",
        "        score[s] = torch.tensor(0.0, dtype=torch.float32)\n",
        "\n",
        "    for u in order:\n",
        "        if torch.isneginf(score[u]):\n",
        "            continue\n",
        "        for a in out_arcs.get(u, []):\n",
        "            cand = score[u] + a.weight\n",
        "            if torch.isneginf(score[a.dst]) or (cand > score[a.dst]):\n",
        "                score[a.dst] = cand\n",
        "                bp[a.dst] = (u, a)\n",
        "\n",
        "    best_accept = None\n",
        "    best_score = neg_inf\n",
        "    for t in g.accept_nodes:\n",
        "        if not torch.isneginf(score[t]) and (best_accept is None or score[t] > best_score):\n",
        "            best_accept = t\n",
        "            best_score = score[t]\n",
        "\n",
        "    if best_accept is None:\n",
        "        return Graph(calc_grad=g.calc_grad)\n",
        "\n",
        "    arcs_rev: List[Arc] = []\n",
        "    cur = best_accept\n",
        "    while bp[cur] is not None:\n",
        "        prev, a = bp[cur]\n",
        "        arcs_rev.append(a)\n",
        "        cur = prev\n",
        "\n",
        "    path_arcs = list(reversed(arcs_rev))\n",
        "    p = Graph(calc_grad=g.calc_grad)\n",
        "    p.add_node(start=True)\n",
        "    for i, a in enumerate(path_arcs, start=1):\n",
        "        p.add_node(accept=(i == len(path_arcs)))\n",
        "        p.add_arc(i - 1, i, a.ilabel, a.olabel, a.weight)\n",
        "    return p\n",
        "\n",
        "\n",
        "# ---------------- Scalar ops and autograd entry ----------------\n",
        "def _scalar_tensor(g: Graph) -> torch.Tensor:\n",
        "    if not g.is_scalar_graph():\n",
        "        raise ValueError(\"Expected scalar graph\")\n",
        "    return g._arcs[0].weight\n",
        "\n",
        "\n",
        "def negate(g: Graph) -> Graph:\n",
        "    return scalar_graph(-_scalar_tensor(g), calc_grad=g.calc_grad)\n",
        "\n",
        "\n",
        "def add(g1: Graph, g2: Graph) -> Graph:\n",
        "    if g1.is_scalar_graph() and g2.is_scalar_graph():\n",
        "        return scalar_graph(_scalar_tensor(g1) + _scalar_tensor(g2), calc_grad=(g1.calc_grad or g2.calc_grad))\n",
        "    raise NotImplementedError(\"Graph-wise add is not implemented; only scalar add is supported.\")\n",
        "\n",
        "\n",
        "def subtract(g1: Graph, g2: Graph) -> Graph:\n",
        "    if g1.is_scalar_graph() and g2.is_scalar_graph():\n",
        "        return scalar_graph(_scalar_tensor(g1) - _scalar_tensor(g2), calc_grad=(g1.calc_grad or g2.calc_grad))\n",
        "    raise NotImplementedError(\"Graph-wise subtract is not implemented; only scalar subtract is supported.\")\n",
        "\n",
        "\n",
        "def backward(g: Graph, retain_graph: bool = False) -> None:\n",
        "    _scalar_tensor(g).backward(retain_graph=retain_graph)\n",
        "\n",
        "\n",
        "# ---------------- Optional namespace shim ----------------\n",
        "class gtn:\n",
        "    Graph = Graph\n",
        "    epsilon = epsilon\n",
        "\n",
        "    scalar_graph = staticmethod(scalar_graph)\n",
        "    linear_graph = staticmethod(linear_graph)\n",
        "\n",
        "    clone = staticmethod(clone)\n",
        "    union = staticmethod(union)\n",
        "    concat = staticmethod(concat)\n",
        "    closure = staticmethod(closure)\n",
        "    compose = staticmethod(compose)\n",
        "    intersect = staticmethod(intersect)\n",
        "\n",
        "    project_input = staticmethod(project_input)\n",
        "    project_output = staticmethod(project_output)\n",
        "    remove = staticmethod(remove)\n",
        "\n",
        "    forward_score = staticmethod(forward_score)\n",
        "    viterbi_score = staticmethod(viterbi_score)\n",
        "    viterbi_path = staticmethod(viterbi_path)\n",
        "\n",
        "    negate = staticmethod(negate)\n",
        "    add = staticmethod(add)\n",
        "    subtract = staticmethod(subtract)\n",
        "    backward = staticmethod(backward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf47dc6",
      "metadata": {},
      "source": [
        "## Quick API sanity checks\n",
        "\n",
        "The next cells verify behavior with GTN-style usage patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67cf6471",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic graph creation + score + backward\n",
        "\n",
        "g1 = gtn.Graph()\n",
        "s0 = g1.add_node(start=True)\n",
        "s1 = g1.add_node(accept=True)\n",
        "g1.add_arc(s0, s1, 1, 1, torch.tensor(0.5, requires_grad=True))\n",
        "\n",
        "g2 = gtn.Graph()\n",
        "t0 = g2.add_node(start=True)\n",
        "t1 = g2.add_node(accept=True)\n",
        "g2.add_arc(t0, t1, 1, 1, torch.tensor(-0.2, requires_grad=True))\n",
        "\n",
        "c = gtn.compose(g1, g2)\n",
        "print(\"compose:\", c)\n",
        "\n",
        "score = gtn.forward_score(c)\n",
        "print(\"score scalar graph:\", score, \"item=\", score.item())\n",
        "\n",
        "loss = gtn.negate(score)\n",
        "gtn.backward(loss)\n",
        "\n",
        "print(\"g1 grad graph weights:\", g1.grad().weights_to_list())\n",
        "print(\"g2 grad graph weights:\", g2.grad().weights_to_list())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba8e252",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional-diacritic WFST (GTN semantics: epsilon = -1)\n",
        "\n",
        "EPSILON = gtn.epsilon\n",
        "\n",
        "def build_arabic_char_with_diacritic_wfst(arabic_chars, diacritics):\n",
        "    g = gtn.Graph(calc_grad=False)\n",
        "\n",
        "    # Keep epsilon separate from regular symbols.\n",
        "    label_map = {\"<eps>\": EPSILON}\n",
        "    next_idx = 0\n",
        "\n",
        "    for label in arabic_chars + diacritics:\n",
        "        if label not in label_map:\n",
        "            while next_idx == EPSILON:\n",
        "                next_idx += 1\n",
        "            label_map[label] = next_idx\n",
        "            next_idx += 1\n",
        "\n",
        "    start_node = g.add_node(start=True)\n",
        "    intermediate_node = g.add_node()\n",
        "    accept_node = g.add_node(accept=True)\n",
        "\n",
        "    for char in arabic_chars:\n",
        "        lab = label_map[char]\n",
        "        g.add_arc(start_node, intermediate_node, lab, lab, 0.0)\n",
        "\n",
        "    for d in diacritics:\n",
        "        lab = label_map[d]\n",
        "        g.add_arc(intermediate_node, accept_node, lab, lab, 0.0)\n",
        "\n",
        "    # Optional skip of diacritic\n",
        "    g.add_arc(intermediate_node, accept_node, EPSILON, EPSILON, 0.0)\n",
        "\n",
        "    return g, label_map\n",
        "\n",
        "arabic_chars = [\"ا\", \"ب\", \"ت\", \"ث\"]\n",
        "diacritics = [\"fatha\", \"damma\", \"kasra\", \"shada\", \"sukun\"]\n",
        "\n",
        "arabic_wfst, label_map = build_arabic_char_with_diacritic_wfst(arabic_chars, diacritics)\n",
        "print(\"label_map:\", label_map)\n",
        "print(arabic_wfst)\n",
        "arabic_wfst.draw(\"arabic_wfst_corrected\", format=\"png\", view=False, label_map=label_map)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab145c9",
      "metadata": {},
      "source": [
        "## Notes for extending toward full parity\n",
        "\n",
        "To move from this notebook implementation to a production-complete GTN alternative, next priorities are:\n",
        "\n",
        "1. Full epsilon-filter composition identical to GTN behavior.\n",
        "2. Batched operations (`parallel_for`, map variants).\n",
        "3. Full criterion module parity (`ctc_loss`, ASG helpers, etc.).\n",
        "4. CUDA kernels / fused semiring operations for performance.\n",
        "5. Serialization helpers and strict test parity against `bindings/python/test`.\n",
        "\n",
        "This notebook now provides a clean, documented, autograd-correct base to build on.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
