{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optional-Diacritic CTC vs Regular CTC (Arabic)\n",
        "\n",
        "This notebook is a comprehensive demonstration of:\n",
        "\n",
        "- **Regular CTC** decoding/training objective\n",
        "- **Optional-diacritic constrained CTC** using a GTN WFST\n",
        "\n",
        "Goal: provide a rigorous, reproducible comparison for research discussion.\n",
        "\n",
        "## What this notebook gives you\n",
        "\n",
        "1. Formal intuition and equations\n",
        "2. A working implementation with Wav2Vec2 Arabic CTC emissions\n",
        "3. Side-by-side scoring (regular vs constrained)\n",
        "4. Decoding and error-analysis hooks\n",
        "5. Evaluation protocol for supervisor-facing evidence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Hypothesis and Positioning\n",
        "\n",
        "### Hypothesis\n",
        "\n",
        "Constrain CTC alignments using a graph that allows optional diacritic insertion after base Arabic characters. This should:\n",
        "\n",
        "- reduce invalid/unlikely orthographic sequences,\n",
        "- improve behavior when diacritics are inconsistently present,\n",
        "- preserve compatibility with CTC acoustic models.\n",
        "\n",
        "### About novelty\n",
        "\n",
        "This notebook demonstrates a technically coherent constrained-CTC formulation. Whether it is *novel* in the publication sense requires literature review against prior graph-constrained CTC/ASR work.\n",
        "\n",
        "Use this notebook to show:\n",
        "- a clear formulation,\n",
        "- reproducible implementation,\n",
        "- empirical deltas under controlled settings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Formulation\n",
        "\n",
        "Let $X$ be acoustic features and $Y$ target tokens.\n",
        "\n",
        "### Regular CTC\n",
        "\n",
        "$$\\mathcal{L}_{CTC}(X,Y) = -\\log \\sum_{\\pi \\in \\mathcal{A}_{CTC}(Y)} p(\\pi|X)$$\n",
        "\n",
        "In GTN terms:\n",
        "\n",
        "- Build CTC target graph $C(Y)$,\n",
        "- Build emissions graph $E(X)$ from frame log-probs,\n",
        "- Score: $-\\mathrm{forward}(C \\circ E)$.\n",
        "\n",
        "### Optional-Diacritic Constrained CTC\n",
        "\n",
        "Introduce WFST $A$ that allows optional diacritic insertion after base characters.\n",
        "\n",
        "$$\\mathcal{L}_{opt}(X,Y) = -\\log \\sum_{\\pi \\in \\mathcal{A}_{CTC}(Y) \\cap \\mathcal{A}_{A}} p(\\pi|X)$$\n",
        "\n",
        "GTN computation:\n",
        "\n",
        "- $C_{opt} = C \\circ A$\n",
        "- loss: $-\\mathrm{forward}(C_{opt} \\circ E)$\n",
        "\n",
        "This preserves CTC-style alignment semantics while restricting/expanding alignments via a linguistic graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import json\n",
        "import random\n",
        "import statistics\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import gtn\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"gtn:\", getattr(gtn, \"__version__\", \"unknown\"))\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional helper deps for large-scale evaluation (uncomment if needed)\n",
        "# !pip install datasets evaluate jiwer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Model and data configuration\n",
        "\n",
        "Set the model id and a sample audio/target pair. For convincing evidence, later run on a held-out set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Replace with your own file and reference\n",
        "AUDIO_PATH = \"path/to/arabic_audio.wav\"\n",
        "TARGET_TEXT = \"سلام\"\n",
        "\n",
        "ARABIC_DIACRITICS = [\"َ\", \"ً\", \"ُ\", \"ٌ\", \"ِ\", \"ٍ\", \"ْ\", \"ّ\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID).to(DEVICE).eval()\n",
        "\n",
        "blank_idx = model.config.pad_token_id\n",
        "if blank_idx is None:\n",
        "    raise RuntimeError(\"No pad_token_id found; cannot infer CTC blank index.\")\n",
        "\n",
        "print(\"Loaded model:\", MODEL_ID)\n",
        "print(\"CTC blank index:\", blank_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Core GTN utilities\n",
        "\n",
        "These are the key building blocks for regular and constrained CTC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio_16k(path: str) -> torch.Tensor:\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if wav.size(0) > 1:\n",
        "        wav = wav.mean(dim=0, keepdim=True)\n",
        "    if sr != 16000:\n",
        "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "    return wav.squeeze(0)\n",
        "\n",
        "\n",
        "def logits_from_wav(wav: torch.Tensor) -> torch.Tensor:\n",
        "    inputs = processor(wav.numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
        "    input_values = inputs.input_values.to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits[0]  # [T, V]\n",
        "    return logits.cpu()\n",
        "\n",
        "\n",
        "def build_emissions_graph(log_probs: np.ndarray) -> gtn.Graph:\n",
        "    t, v = log_probs.shape\n",
        "    g = gtn.linear_graph(t, v, calc_grad=False)\n",
        "    g.set_weights(log_probs.astype(np.float32).reshape(-1))\n",
        "    return g\n",
        "\n",
        "\n",
        "def create_ctc_target_graph(target_ids: List[int], blank_idx: int) -> gtn.Graph:\n",
        "    l = len(target_ids)\n",
        "    u = 2 * l + 1\n",
        "    g = gtn.Graph(False)\n",
        "    for p in range(u):\n",
        "        idx = (p - 1) // 2\n",
        "        g.add_node(p == 0, p == u - 1 or p == u - 2)\n",
        "        label = target_ids[idx] if (p % 2) else blank_idx\n",
        "        g.add_arc(p, p, label)\n",
        "        if p > 0:\n",
        "            g.add_arc(p - 1, p, label)\n",
        "        if p % 2 and p > 1 and label != target_ids[idx - 1]:\n",
        "            g.add_arc(p - 2, p, label)\n",
        "    return g\n",
        "\n",
        "\n",
        "def build_optional_diacritic_wfst(\n",
        "    ctc_symbols: List[int],\n",
        "    base_symbol_ids: List[int],\n",
        "    diacritic_ids: List[int],\n",
        ") -> gtn.Graph:\n",
        "    g = gtn.Graph(False)\n",
        "    s0 = g.add_node(True, True)\n",
        "    s1 = g.add_node(False, False)\n",
        "\n",
        "    base_set = set(base_symbol_ids)\n",
        "\n",
        "    # Pass-through for all symbols; base chars transition into diacritic state\n",
        "    for sym in ctc_symbols:\n",
        "        if sym in base_set:\n",
        "            g.add_arc(s0, s1, sym, sym, 0.0)\n",
        "        else:\n",
        "            g.add_arc(s0, s0, sym, sym, 0.0)\n",
        "\n",
        "    # Optional insertion: epsilon input -> diacritic output\n",
        "    for d in diacritic_ids:\n",
        "        g.add_arc(s1, s0, gtn.epsilon, d, 0.0)\n",
        "\n",
        "    # Optional skip (no diacritic)\n",
        "    g.add_arc(s1, s0, gtn.epsilon, gtn.epsilon, 0.0)\n",
        "    return g\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Single-utterance demonstration\n",
        "\n",
        "Compute both losses from the same emissions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_file = Path(AUDIO_PATH)\n",
        "if not audio_file.exists():\n",
        "    raise FileNotFoundError(f\"Update AUDIO_PATH first: {audio_file}\")\n",
        "\n",
        "wav = load_audio_16k(str(audio_file))\n",
        "logits = logits_from_wav(wav)\n",
        "log_probs = torch.log_softmax(logits, dim=-1).numpy()  # [T, V]\n",
        "\n",
        "# Reference text -> tokenizer ids\n",
        "target_ids = processor.tokenizer(TARGET_TEXT, add_special_tokens=False).input_ids\n",
        "if not target_ids:\n",
        "    raise ValueError(\"Target text tokenized to empty sequence.\")\n",
        "\n",
        "# Build regular CTC loss\n",
        "E = build_emissions_graph(log_probs)\n",
        "C = create_ctc_target_graph(target_ids, blank_idx=blank_idx)\n",
        "regular_loss = gtn.negate(gtn.forward_score(gtn.compose(C, E))).item()\n",
        "\n",
        "# Build constrained CTC loss\n",
        "vocab = processor.tokenizer.get_vocab()\n",
        "diacritic_ids = sorted({vocab[d] for d in ARABIC_DIACRITICS if d in vocab})\n",
        "ctc_symbols = sorted(set([blank_idx] + target_ids))\n",
        "\n",
        "A = build_optional_diacritic_wfst(ctc_symbols, target_ids, diacritic_ids)\n",
        "C_opt = gtn.compose(C, A)\n",
        "opt_loss = gtn.negate(gtn.forward_score(gtn.compose(C_opt, E))).item()\n",
        "\n",
        "print(\"Target text             :\", TARGET_TEXT)\n",
        "print(\"Target ids              :\", target_ids)\n",
        "print(\"Diacritic ids in vocab  :\", diacritic_ids)\n",
        "print(f\"Regular CTC loss        : {regular_loss:.4f}\")\n",
        "print(f\"Optional-Diacritic loss : {opt_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Greedy decode reference (model output, independent from GTN scoring)\n",
        "pred_ids = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "pred_text = processor.batch_decode(pred_ids)[0]\n",
        "print(\"Greedy model decode:\", pred_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Visualization hook (constraint graph)\n",
        "\n",
        "Export the optional-diacritic WFST to DOT for inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
        "symbols = {i: inv_vocab.get(i, str(i)) for i in sorted(set(ctc_symbols + diacritic_ids))}\n",
        "symbols[gtn.epsilon] = \"ε\"\n",
        "\n",
        "out_dot = \"optional_diacritic_wfst.dot\"\n",
        "gtn.draw(A, out_dot, symbols, symbols)\n",
        "print(\"Wrote:\", out_dot)\n",
        "print(\"Tip: dot -Tpng optional_diacritic_wfst.dot -o optional_diacritic_wfst.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Batch evaluation protocol (for convincing evidence)\n",
        "\n",
        "For a supervisor-ready comparison, do this on a held-out Arabic set:\n",
        "\n",
        "1. Fix model checkpoint and preprocessing.\n",
        "2. Use same references for both conditions.\n",
        "3. Compute per-utterance regular and constrained losses.\n",
        "4. Decode and compare CER/WER (or Arabic-normalized CER).\n",
        "5. Run paired significance tests.\n",
        "\n",
        "Recommended outputs:\n",
        "- table of mean/median loss delta,\n",
        "- CER/WER deltas,\n",
        "- examples where constrained CTC helps/hurts,\n",
        "- ablations on diacritic set size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class UtteranceResult:\n",
        "    uid: str\n",
        "    target: str\n",
        "    regular_loss: float\n",
        "    opt_loss: float\n",
        "    pred: str\n",
        "\n",
        "\n",
        "def score_one(audio_path: str, target_text: str) -> UtteranceResult:\n",
        "    wav = load_audio_16k(audio_path)\n",
        "    logits = logits_from_wav(wav)\n",
        "    log_probs = torch.log_softmax(logits, dim=-1).numpy()\n",
        "\n",
        "    target_ids = processor.tokenizer(target_text, add_special_tokens=False).input_ids\n",
        "    if not target_ids:\n",
        "        raise ValueError(f\"Empty target tokenization for: {target_text}\")\n",
        "\n",
        "    E = build_emissions_graph(log_probs)\n",
        "    C = create_ctc_target_graph(target_ids, blank_idx)\n",
        "\n",
        "    reg = gtn.negate(gtn.forward_score(gtn.compose(C, E))).item()\n",
        "\n",
        "    ctc_symbols = sorted(set([blank_idx] + target_ids))\n",
        "    A = build_optional_diacritic_wfst(ctc_symbols, target_ids, diacritic_ids)\n",
        "    C_opt = gtn.compose(C, A)\n",
        "    opt = gtn.negate(gtn.forward_score(gtn.compose(C_opt, E))).item()\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "    pred = processor.batch_decode(pred_ids)[0]\n",
        "\n",
        "    return UtteranceResult(\n",
        "        uid=Path(audio_path).stem,\n",
        "        target=target_text,\n",
        "        regular_loss=reg,\n",
        "        opt_loss=opt,\n",
        "        pred=pred,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example manifest format:\n",
        "# manifest = [\n",
        "#   {\"audio\": \"/path/a.wav\", \"text\": \"...\"},\n",
        "#   {\"audio\": \"/path/b.wav\", \"text\": \"...\"},\n",
        "# ]\n",
        "\n",
        "manifest = []  # fill this with your evaluation set\n",
        "\n",
        "results = []\n",
        "for ex in manifest:\n",
        "    try:\n",
        "        results.append(score_one(ex[\"audio\"], ex[\"text\"]))\n",
        "    except Exception as e:\n",
        "        print(\"failed:\", ex.get(\"audio\", \"?\"), str(e))\n",
        "\n",
        "if results:\n",
        "    deltas = [r.opt_loss - r.regular_loss for r in results]\n",
        "    print(\"N:\", len(results))\n",
        "    print(\"Mean delta (opt - regular):\", float(np.mean(deltas)))\n",
        "    print(\"Median delta:\", float(np.median(deltas)))\n",
        "    print(\"Min/Max delta:\", float(np.min(deltas)), float(np.max(deltas)))\n",
        "else:\n",
        "    print(\"No results yet. Fill manifest and rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Ablation matrix\n",
        "\n",
        "Run these to strengthen your case:\n",
        "\n",
        "1. **No constraint** (regular CTC baseline)\n",
        "2. **Optional all diacritics** (current)\n",
        "3. **Optional subset of diacritics**\n",
        "4. **Penalty-weighted insertions** (non-zero arc penalties)\n",
        "5. **Constraint only at decode** vs **constraint in training loss**\n",
        "\n",
        "Track:\n",
        "- CER/WER\n",
        "- diacritic-specific error rates\n",
        "- loss curves and calibration\n",
        "- runtime overhead (composition/scoring)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Supervisor-ready summary template\n",
        "\n",
        "Use this structure in your report/slides:\n",
        "\n",
        "1. Problem: Arabic diacritics are optional/inconsistent in labels.\n",
        "2. Method: Constrained CTC using optional-diacritic WFST in GTN.\n",
        "3. Theory: CTC target graph composed with linguistic graph.\n",
        "4. Implementation: Wav2Vec2 emissions + GTN graph scoring.\n",
        "5. Results: paired comparison against same checkpoint/data.\n",
        "6. Analysis: when and why constraints help; failure cases.\n",
        "7. Claim scope: method is technically valid; novelty requires literature-grounded positioning.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
